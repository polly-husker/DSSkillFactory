проект на kaggle 
https://www.kaggle.com/c/sf-dst-car-price-prediction-part2/overview

кратко: задача предсказания стоимости машин, имеются табличные данные, текстовые описания объявлений, фото машин.
Строится модель catboost по табличным данным
строится нейросеть в мульти-входами - таблицы, текст (nlp (LSTM), картинки )
Итоговое предсказание - бленд описанных двух моделей, с округлением до целых тысяч (стоимость машины)


разработка велась в колабе
https://colab.research.google.com/drive/1sFb3z2IUKkU3OUsiuMlrbwxEQ1h4IpKH?authuser=0#scrollTo=MQPM9yFr_orX

Когда в колабе закончились часы, то модели перенеслись опять в кагл и доделывалось там, опять перескакивая в колаб для проверок, МРАК О_о :P

в файл desc_lemmatized записано данные, которые получились после лемматизации текстовых описаний

Выводы и рефлексия:
Использованные приемы бленда, мульти-входа для nn - классные штуки, которые дают прирост качества модели, при этом временные затраты на сам просчет модели почти без изменений. Возможно картина была б другой, если попробовать для nlp BERT. Но в этой сети самым затратным была часть с картинками, и честно говоря не хватило сил и времени ее раскрутить так же качественно как в прошлом проекте. По той же причине не опробован проброс признака - хотелось увидеть как получилось улучшить бейслайн без проброса, проброс остался у обочины.
Остались пропуски в моменте - как помочь LSTM'y при работе с токенизацией (как рассказать про словосочетания, как прочистить word_index - и КАК и ПО КАКОМУ ПРИНЦИПУ - типа: надо ли убирать "с", "в", "на", числа, убирать сразу в основных текстах или после токенизации)